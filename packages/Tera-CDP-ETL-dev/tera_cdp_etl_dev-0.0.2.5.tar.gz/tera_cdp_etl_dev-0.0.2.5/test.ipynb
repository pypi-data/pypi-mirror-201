{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"UserId\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"name\": \"UserIdHD\",\n",
    "        #     \"fieldType\": \"string\",\n",
    "        #     \"comment\": \"\",\n",
    "        #     \"validation_rule\": \"\"\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"HomePhone\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"WorkPhone\",\n",
    "            \"fieldType\": [\n",
    "                \"string\",\n",
    "                \"null\"\n",
    "            ],\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Address\",\n",
    "            \"fieldType\": \"object\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\",\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"name\": \"HouseNo\",\n",
    "                    \"fieldType\": \"string\",\n",
    "                    \"comment\": \"\",\n",
    "                    \"validation_rule\": \"\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Streetname\",\n",
    "                    \"fieldType\": \"string\",\n",
    "                    \"comment\": \"\",\n",
    "                    \"validation_rule\": \"\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Ward\",\n",
    "                    \"fieldType\": \"string\",\n",
    "                    \"comment\": \"\",\n",
    "                    \"validation_rule\": \"\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"District\",\n",
    "                    \"fieldType\": \"string\",\n",
    "                    \"comment\": \"\",\n",
    "                    \"validation_rule\": \"\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Province\",\n",
    "                    \"fieldType\": \"string\",\n",
    "                    \"comment\": \"\",\n",
    "                    \"validation_rule\": \"\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Country\",\n",
    "                    \"fieldType\": \"string\",\n",
    "                    \"comment\": \"\",\n",
    "                    \"validation_rule\": \"\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Firstname\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Lastname\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Gender\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RegisterDate\",\n",
    "            \"fieldType\": \"datetime\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Status\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DataSource\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Source\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EventId\",\n",
    "            \"fieldType\": \"string\",\n",
    "            \"comment\": \"\",\n",
    "            \"validation_rule\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"UserId\": 202201439424,\n",
    "    \"HomePhone\": \"0912919543\",\n",
    "    \"WorkPhone\": None,\n",
    "    \"Address\": {\n",
    "        \"HouseNo\": \"\",\n",
    "        \"Streetname\": \"\",\n",
    "        \"Ward\": \"ẤP HÒA CƯỜNG, Xã Minh Hoà\",\n",
    "        \"District\": \"Huyện Dầu Tiếng\",\n",
    "        \"Province\": \"Tỉnh Bình Dương\",\n",
    "        \"Country\": \"Việt Nam\"\n",
    "    },\n",
    "    \"Firstname\": \"TRẦN VĂN \",\n",
    "    \"Lastname\": \"TÙNG\",\n",
    "    \"Gender\": \"Male\",\n",
    "    \"RegisterDate\": \"2022-01-01T00:00:00.00Z\",\n",
    "    \"Status\": \"True\",\n",
    "    \"DataSource\": \"VTVHyundai\",\n",
    "    \"Source\": \"VTVHyundai\",\n",
    "    \"EventId\": \"VTVHyundai_IngestProfile_202201439424\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airflow.dags.schema_validator as schema_validator\n",
    "def build_path(path, key):\n",
    "    return key if len(path) == 0 else path + '.' + key\n",
    "def get_default_value(m_key):\n",
    "    # Model field is an array of objects\n",
    "    # case1:     foo: [{bar: ['string', 'value']}]\n",
    "    if type(m_key) is list and type(m_key[0]) is dict:\n",
    "        return []\n",
    "\n",
    "    # Model field is an object\n",
    "    # case2:     foo: {bar: ['string', 'value']}\n",
    "    if type(m_key) is dict:\n",
    "        res = {}\n",
    "        for key in m_key.keys():\n",
    "            res[key] = get_default_value(m_key[key])\n",
    "        return res\n",
    "\n",
    "    # case3:     foo: ['string', 'value']\n",
    "    return m_key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"Fields\": [\n",
    "      {\"Name\": \"UserId\", \"FieldType\": \"string\", \"Validation\": \"String,MustHave\", \"Comment\": \"id của user trong hệ thống, phải là kiểu string\" },\n",
    "      {\"Name\": \"HomePhone\", \"FieldType\": \"string\", \"Comment\": \"Nếu không có thì để trống\" },\n",
    "      {\"Name\": \"WorkPhone\", \"FieldType\": [\"string\", \"null\"], \"Comment\": \"Nếu không có thì để trống\" },\n",
    "      {\"Name\": \"AddrHouseNo\", \"FieldType\": \"string\", \"Comment\": \"Số nhà\" },\n",
    "      {\"Name\": \"AddrStreetName\", \"FieldType\": \"string\", \"Comment\": \"Tên đường\" },\n",
    "      {\"Name\": \"AddrWard\", \"FieldType\": \"string\", \"Comment\": \"Phường / Xã\" },\n",
    "      {\"Name\": \"AddrDistrict\", \"FieldType\": \"string\", \"Comment\": \"Quận / Huyện / Thành Phố trực thuộc tỉnh\" },\n",
    "      {\"Name\": \"AddrProvince\", \"FieldType\": \"string\", \"Comment\": \"Tỉnh / Thành phố trực thuộc TW\" },\n",
    "      {\"Name\": \"AddrCountry\", \"FieldType\": \"string\", \"Validation\": \"Iso Country Code\", \"Comment\": \"Tên nước - ISO Country Code - E.g. VN\" },\n",
    "      {\"Name\": \"Firstname\", \"FieldType\": \"string\", \"Comment\": \"Tên\" },\n",
    "      {\"Name\": \"Lastname\", \"FieldType\": \"string\", \"Comment\": \"Họ và chữ lót\" },\n",
    "      {\"Name\": \"Gender\", \"FieldType\": \"string\", \"Comment\": \"Giới tính, tiếng anh, để khỏi bị nhầm lẫn\", \"Validation\": \"Male/Female/Other\" },\n",
    "      {\"Name\": \"RegisterDate\", \"FieldType\": \"string\", \"Comment\": \"Ngày đăng ký vào hệ thống\" },\n",
    "      {\"Name\": \"DataSource\", \"FieldType\": \"string\", \"Comment\": \"DataProviderName\" },\n",
    "      {\"Name\": \"MaritalStatus\", \"FieldType\": \"string\", \"Comment\": \"Tình trạng hôn nhân\", \"Validation\": \"Single/Married\" }\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors = schema_validator.validate_v2(schema,data)\\\n",
    "schema_dict = {}\n",
    "for field in schema[\"fields\"]:\n",
    "    if field[\"fieldType\"] == \"object\":\n",
    "        schema_dict[field[\"name\"]] = {}\n",
    "        for property in field[\"properties\"]:\n",
    "            schema_dict[field[\"name\"]][property[\"name\"]] = property[\"fieldType\"]\n",
    "    else:\n",
    "        schema_dict[field[\"name\"]] = field[\"fieldType\"]\n",
    "path = \"\"\n",
    "result = []\n",
    "print(schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for field in data:\n",
    "    if field not in list_field_name:\n",
    "        fullpath = build_path(path, field)\n",
    "        field_type = type(data[field]).__name__\n",
    "        error_response = {\n",
    "            'msg': '[+] Extra field: \"{}\" having type: \"{}\"'.format(fullpath, field_type),\n",
    "            'type': 'extra_field',\n",
    "            'path': fullpath,\n",
    "            'field_type': field_type\n",
    "        }\n",
    "        result.append(error_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_field_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_field = schema[\"fields\"][]\n",
    "for key in list_field_name:\n",
    "    if key not in data:\n",
    "        fullpath = build_path(path, key)\n",
    "        error_response = {\n",
    "            'msg': '[-] Missing field: \"{}\"'.format(fullpath),\n",
    "            'type': 'missing_field',\n",
    "            'path': fullpath,\n",
    "            'default_value': get_default_value([key])\n",
    "        }\n",
    "        result.append(error_response)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"UserId\" : \"int\",\n",
    "    \"HomePhone\" : \"string\",\n",
    "    \"WorkPhone\" : [\"string\", \"null\"],\n",
    "    \"Address\" : {\n",
    "        \"HouseNo\": \"string\",\n",
    "        \"Streetname\": \"string\",\n",
    "        \"Ward\": \"string\",\n",
    "        \"District\": \"string\",\n",
    "        \"Province\": \"string\",\n",
    "        \"Country\": \"string\"\n",
    "    },\n",
    "    \"Firstname\" : \"string\",\n",
    "    \"Lastname\" : \"string\",\n",
    "    \"Gender\" : \"string\",\n",
    "    \"RegisterDate\" : \"datetime\",\n",
    "    \"Status\" : \"string\",\n",
    "    \"DataSource\" : \"string\",\n",
    "    \"Source\" : \"string\",\n",
    "    \"EventId\" : \"string\"\n",
    "}\n",
    "data = {\n",
    "    \"UserId\": 202201439424,\n",
    "    \"HomePhone\": \"0912919543\",\n",
    "    \"WorkPhone\": None,\n",
    "    \"Address\": {\n",
    "        \"HouseNo\": \"\",\n",
    "        \"Streetname\": \"\",\n",
    "        \"Ward\": \"ẤP HÒA CƯỜNG, Xã Minh Hoà\",\n",
    "        \"District\": \"Huyện Dầu Tiếng\",\n",
    "        \"Province\": \"Tỉnh Bình Dương\",\n",
    "        \"Country\": \"Việt Nam\"\n",
    "    },\n",
    "    \"Firstname\": \"TRẦN VĂN \",\n",
    "    \"Lastname\": \"TÙNG\",\n",
    "    \"Gender\": \"Male\",\n",
    "    \"RegisterDate\": \"2022-01-01T00:00:00.00Z\",\n",
    "    \"Status\": \"True\",\n",
    "    \"DataSource\": \"VTVHyundai\",\n",
    "    \"Source\": \"VTVHyundai\",\n",
    "    \"EventId\": \"VTVHyundai_IngestProfile_202201439424\"\n",
    "}\n",
    "a = schema_validator.validate(schema,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import os\n",
    "class S3BucketConfig(Enum):\n",
    "    QC_BUCKET = 'teracdp-data-quality'\n",
    "    QC_ACCEPT_PATH = 'accept'\n",
    "    QC_REJECT_PATH = 'reject'\n",
    "print(S3BucketConfig.QC_BUCKET.value)\n",
    "print(os.path.join(*[S3BucketConfig.QC_ACCEPT_PATH.value, \"data_source\", \"uploaded_date\", \"event_name\", \"filename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Attr, And, Key\n",
    "from functools import reduce\n",
    "DYNAMODB_DATA_SCHEMA_TABLE = 'SchemaRegistry'\n",
    "DYNAMODB_DATA_PROVIDER_SCHEMA = 'DataProviderSchema'\n",
    "DYNAMODB_DATA_QUALITY_CONTROL_LOG = 'QualityControlLog'\n",
    "dynamodb_resource = boto3.resource(\n",
    "        'dynamodb',\n",
    "        aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "        aws_secret_access_key=\"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\",\n",
    "        region_name=\"ap-southeast-1\",\n",
    "    )\n",
    "table = dynamodb_resource.Table(DYNAMODB_DATA_PROVIDER_SCHEMA) \n",
    "attribs = {'DataProvider':\"VTVHuyndai\", 'EventName':\"IngestProfile\"}\n",
    "response = table.scan(\n",
    "    FilterExpression=(\n",
    "        reduce(And, [Key(k).eq(v) for k, v in attribs.items()])\n",
    "    )\n",
    ")\n",
    "rows = response.get('Items')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tera_etl import quality_control as qc\n",
    "schema = {\n",
    "    \"UserId\" : \"int\",\n",
    "    \"HomePhone\" : \"string\",\n",
    "    \"WorkPhone\" : [\"string\", \"null\"],\n",
    "    \"Address\" : {\n",
    "        \"HouseNo\": \"string\",\n",
    "        \"Streetname\": \"string\",\n",
    "        \"Ward\": \"string\",\n",
    "        \"District\": \"string\",\n",
    "        \"Province\": \"string\",\n",
    "        \"Country\": \"string\"\n",
    "    },\n",
    "    \"Firstname\" : \"string\",\n",
    "    \"Lastname\" : \"string\",\n",
    "    \"Gender\" : \"string\",\n",
    "    \"RegisterDate\" : \"datetime\",\n",
    "    \"Status\" : \"string\",\n",
    "    \"DataSource\" : \"string\",\n",
    "    \"Source\" : \"string\",\n",
    "    \"EventId\" : \"string\"\n",
    "}\n",
    "data = {\n",
    "    \"UserId\": 202201439424,\n",
    "    \"HomePhone\": \"0912919543\",\n",
    "    \"WorkPhone\": None,\"\"\n",
    "    \"Address\": {\n",
    "        \"HouseNo\": \"\",\n",
    "        \"Streetname\": \"\",\n",
    "        \"Ward\": \"ẤP HÒA CƯỜNG, Xã Minh Hoà\",\n",
    "        \"District\": \"Huyện Dầu Tiếng\",\n",
    "        \"Province\": \"Tỉnh Bình Dương\",\n",
    "        \"Country\": \"Việt Nam\"\n",
    "    },\n",
    "    \"Firstname\": \"TRẦN VĂN \",\n",
    "    \"Lastname\": \"TÙNG\",\n",
    "    \"Gender\": \"Male\",\n",
    "    \"RegisterDate\": \"2022-01-01T00:00:00.00Z\",\n",
    "    \"DataSource\": \"VTVHyundai\",\n",
    "    \"MaritalStatus\": \"Single\",\n",
    "}\n",
    "\n",
    "structures = {\n",
    "    \"Fields\": [\n",
    "      {\"Name\": \"UserId\", \"FieldType\": \"string\", \"Validation\": \"String,MustHave\", \"Comment\": \"id của user trong hệ thống, phải là kiểu string\" },\n",
    "      {\"Name\": \"HomePhone\", \"FieldType\": [\"string\", \"null\"], \"Comment\": \"Nếu không có thì để trống\" },\n",
    "      {\"Name\": \"WorkPhone\", \"FieldType\": [\"string\", \"null\"], \"Comment\": \"Nếu không có thì để trống\" },\n",
    "      {\"Name\": \"AddrHouseNo\", \"FieldType\": \"string\", \"Comment\": \"Số nhà\" },\n",
    "      {\"Name\": \"AddrStreetName\", \"FieldType\": \"string\", \"Comment\": \"Tên đường\" },\n",
    "      {\"Name\": \"AddrWard\", \"FieldType\": \"string\", \"Comment\": \"Phường / Xã\" },\n",
    "      {\"Name\": \"AddrDistrict\", \"FieldType\": \"string\", \"Comment\": \"Quận / Huyện / Thành Phố trực thuộc tỉnh\" },\n",
    "      {\"Name\": \"AddrProvince\", \"FieldType\": \"string\", \"Comment\": \"Tỉnh / Thành phố trực thuộc TW\" },\n",
    "      {\"Name\": \"AddrCountry\", \"FieldType\": \"string\", \"Validation\": \"Iso Country Code\", \"Comment\": \"Tên nước - ISO Country Code - E.g. VN\" },\n",
    "      {\"Name\": \"Firstname\", \"FieldType\": \"string\", \"Comment\": \"Tên\" },\n",
    "      {\"Name\": \"Lastname\", \"FieldType\": \"string\", \"Comment\": \"Họ và chữ lót\" },\n",
    "      {\"Name\": \"Gender\", \"FieldType\": \"string\", \"Comment\": \"Giới tính, tiếng anh, để khỏi bị nhầm lẫn\", \"Validation\": \"Male/Female/Other\" },\n",
    "      {\"Name\": \"RegisterDate\", \"FieldType\": \"datetime\", \"Comment\": \"Ngày đăng ký vào hệ thống\" },\n",
    "      {\"Name\": \"DataSource\", \"FieldType\": \"string\", \"Comment\": \"DataProviderName\" },\n",
    "      {\"Name\": \"MaritalStatus\", \"FieldType\": \"string\", \"Comment\": \"Tình trạng hôn nhân\", \"Validation\": \"Single/Married\" }\n",
    "    ]\n",
    "  }\n",
    "data = {\n",
    "    \"UserId\": 202201439260,\n",
    "    \"HomePhone\": \"0369841490\",\n",
    "    \"WorkPhone\": None,\n",
    "    \"AddrHouseNo\": \"\",\n",
    "    \"AddrStreetname\": \"\",\n",
    "    \"AddrWard\": \"15 NGÕ 341, Phường Xuân Phương\",\n",
    "    \"AddrDistrict\": \"Quận Nam Từ Liêm\",\n",
    "    \"AddrProvince\": \"Thành phố Hà Nội\",\n",
    "    \"AddrCountry\": \"Việt Nam\",\n",
    "    \"Firstname\": \"NGUYỄN THỊ BÍCH \",\n",
    "    \"Lastname\": \"HIỀN\",\n",
    "    \"Gender\": \"Female\",\n",
    "    \"RegisterDate\": \"2022-01-01T00:00:00.00Z\",\n",
    "    \"DataSource\": \"VTVHyundai\",\n",
    "    \"MaritalStatus\": \"Single\"\n",
    "}\n",
    "schema = {\n",
    "    'UserId': 'string',\n",
    "    'HomePhone': ['string', 'null'],\n",
    "    'WorkPhone': ['string', 'null'],\n",
    "    'AddrHouseNo': 'string',\n",
    "    'AddrStreetName': 'string',\n",
    "    'AddrWard': 'string',\n",
    "    'AddrDistrict': 'string',\n",
    "    'AddrProvince': 'string',\n",
    "    'AddrCountry': 'string',\n",
    "    'Firstname': 'string',\n",
    "    'Lastname': 'string',\n",
    "    'Gender': 'string',\n",
    "    'RegisterDate': 'datetime',\n",
    "    'DataSource': 'string',\n",
    "    'MaritalStatus': 'string'\n",
    "}\n",
    "\n",
    "# schema_dict={}\n",
    "# for field in structures[\"Fields\"]:\n",
    "#     schema_dict[field[\"Name\"]] = field[\"FieldType\"]\n",
    "# print(schema_dict)\n",
    "qc_type = qc.classify_data(data_chunk=data, schema=schema)\n",
    "print(qc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_dict = {}\n",
    "for field in structures[\"Fields\"]:\n",
    "    # if field[\"fieldType\"] == \"object\":\n",
    "    #     schema_dict[field[\"name\"]] = {}\n",
    "    #     for property in field[\"properties\"]:\n",
    "    #         schema_dict[field[\"name\"]][property[\"name\"]] = property[\"fieldType\"]\n",
    "    # else:\n",
    "    schema_dict[field[\"Name\"]] = field[\"FieldType\"]\n",
    "print(schema_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class QualityControlStatus(Enum):\n",
    "    SUCCESS = 'SUCCESS'\n",
    "    FAILURE = 'FAILURE'\n",
    "    PARTIAL = 'PARTIAL'\n",
    "\n",
    "status = QualityControlStatus.SUCCESS\n",
    "if status != QualityControlStatus.FAILURE:\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_schema = \"/Users/mac/Downloads/IngestProfile_20230214.json\"\n",
    "f = open(data_schema)\n",
    "datas = json.load(f)\n",
    "f.close()\n",
    "res = []\n",
    "for data in datas:\n",
    "    row = {}\n",
    "    for k,v in data.items():\n",
    "        if k in [\"Status\", \"Source\", \"EventId\"]:\n",
    "            continue\n",
    "        elif k == \"Address\":\n",
    "            for k1,v1 in v.items():\n",
    "                if k1 == \"Streetname\":\n",
    "                    row[f\"AddrStreetName\"] = v1\n",
    "                else:\n",
    "                    row[f\"Addr{k1}\"] = v1\n",
    "        elif k == \"UserId\":\n",
    "            row[k] = str(v)\n",
    "        else:\n",
    "            row[k] = v \n",
    "    row[\"MaritalStatus\"] = \"Single\"\n",
    "    res.append(row)\n",
    "# print(res)\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(res, f, indent=4, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tera_etl import quality_control as qc\n",
    "from tera_etl import normalise_data as nd\n",
    "import json\n",
    "schema = {\n",
    "    'UserId': 'string',\n",
    "    'HomePhone': ['string', 'null'],\n",
    "    'WorkPhone': ['string', 'null'],\n",
    "    'AddrHouseNo': 'string',\n",
    "    \"AddrStreetName\": \"string\",\n",
    "    'AddrWard': 'string',\n",
    "    'AddrDistrict': 'string',\n",
    "    'AddrProvince': 'string',\n",
    "    'AddrCountry': 'string',\n",
    "    'Firstname': 'string',\n",
    "    'Lastname': 'string',\n",
    "    'Gender': 'string',\n",
    "    'RegisterDate': 'datetime',\n",
    "    'DataSource': 'string',\n",
    "    'MaritalStatus': 'string'\n",
    "}\n",
    "f = open(\"data.json\")\n",
    "datas = json.load(f)\n",
    "f.close()\n",
    "a = nd.normalise_process(datas, \"UserProfileSchema\")\n",
    "print(a)\n",
    "# for data in datas:\n",
    "    # print(data)\n",
    "    # qc_result = qc.classify_data(data_chunk=data, schema=schema)\n",
    "    # if qc_result[\"qc_type\"] == qc.QualityControlResult.ACCEPTED:\n",
    "    #     # print(qc_result[\"qc_type\"])\n",
    "    #     pass\n",
    "    # else:\n",
    "    #     print(qc_result[\"errors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class QualityControlResult(Enum):\n",
    "    ACCEPTED = 'ACCEPTED'\n",
    "    REJECTED = 'REJECTED'\n",
    "\n",
    "\n",
    "def classify_data(data_chunk, schema, **kwargs) -> QualityControlResult:\n",
    "    is_accepted = __is_accepted(data_chunk, schema)\n",
    "    return {\n",
    "        \"qc_type\": QualityControlResult.ACCEPTED if is_accepted[\"status\"] else QualityControlResult.REJECTED,\n",
    "        \"errors\": []\n",
    "    }\n",
    "\n",
    "def __is_accepted(chunk, schema):\n",
    "    return __has_valid_schema(chunk, schema)\n",
    "\n",
    "\n",
    "def __has_valid_schema(data, schema):\n",
    "    return {\n",
    "        \"status\": True,\n",
    "        \"errors\": [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_result = classify_data(data_chunk=data, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_result[\"qc_type\"] == QualityControlResult.ACCEPTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = {\n",
    "    \"Fields\": [\n",
    "      {\"Name\": \"UserId\", \"FieldType\": \"string\", \"Required\": True, \"Nullable\": False, \"Comment\": \"id của user trong hệ thống, phải là kiểu string\" },\n",
    "      {\"Name\": \"HomePhone\", \"FieldType\": \"string\", \"Comment\": \"Nếu không có thì để trống\" },\n",
    "      {\"Name\": \"WorkPhone\", \"FieldType\": \"string\", \"Comment\": \"Nếu không có thì để trống\" },\n",
    "      {\"Name\": \"AddrHouseNo\", \"FieldType\": \"string\", \"Comment\": \"Số nhà\" },\n",
    "      {\"Name\": \"AddrStreetName\", \"FieldType\": \"string\", \"Comment\": \"Tên đường\" },\n",
    "      {\"Name\": \"AddrWard\", \"FieldType\": \"string\", \"Comment\": \"Phường / Xã\" },\n",
    "      {\"Name\": \"AddrDistrict\", \"FieldType\": \"string\", \"Comment\": \"Quận / Huyện / Thành Phố trực thuộc tỉnh\" },\n",
    "      {\"Name\": \"AddrProvince\", \"FieldType\": \"string\", \"Comment\": \"Tỉnh / Thành phố trực thuộc TW\" },\n",
    "      {\"Name\": \"AddrCountry\", \"FieldType\": \"string\", \"Validation\": \"Iso Country Code\", \"Comment\": \"Tên nước - ISO Country Code - E.g. VN\" },\n",
    "      {\"Name\": \"Firstname\", \"FieldType\": \"string\", \"Comment\": \"Tên\" },\n",
    "      {\"Name\": \"Lastname\", \"FieldType\": \"string\", \"Comment\": \"Họ và chữ lót\" },\n",
    "      {\"Name\": \"Gender\", \"FieldType\": \"string\", \"Comment\": \"Giới tính, tiếng anh, để khỏi bị nhầm lẫn\", \"Validation\": \"Male/Female/Other\" },\n",
    "      {\"Name\": \"RegisterDate\", \"FieldType\": \"datetime\", \"Comment\": \"Ngày đăng ký vào hệ thống\" },\n",
    "      {\"Name\": \"DataSource\", \"FieldType\": \"string\", \"Comment\": \"DataProviderName\" },\n",
    "      {\"Name\": \"MaritalStatus\", \"FieldType\": \"string\", \"Comment\": \"Tình trạng hôn nhân\", \"Validation\": \"Single/Married\" }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_camel_case(s):\n",
    "    return s != s.lower() and s != s.upper() and \"_\" not in s\n",
    "\n",
    "\n",
    "tests = [\n",
    "    \"camel\",\n",
    "    \"123213camelCase12312\",\n",
    "    \"CamelCase\",\n",
    "    \"CAMELCASE\",\n",
    "    \"camelcase\",\n",
    "    \"Camelcase\",\n",
    "    \"Case\",\n",
    "    \"camel_case\",\n",
    "    \"CAmelCase\",\n",
    "    \"VTVHyunhdai\",\n",
    "    \"VTVCab\",\n",
    "    \"dasdasd_asdasda\",\n",
    "]\n",
    "\n",
    "for test in tests:\n",
    "    print(test, is_camel_case(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert(CamelCase):\n",
    "    string = re.sub('(.)([A-Z][a-z]+)', r'\\1-\\2', CamelCase)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1-\\2', string).lower()\n",
    "for test in tests:\n",
    "    print(test, convert(test))\n",
    "# for test in tests:\n",
    "#     if is_camel_case(test):\n",
    "#         print(test)\n",
    "#         print(re.findall(r'[A-Za-z](?:[a-z]+|[A-Z]*(?=[AZ]|$))', test))\n",
    "# name = 'CamelCaseName'\n",
    "        # name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "        # name =  re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()\n",
    "        # name = re.sub(r'(?<!^)(?=[A-Z])', '-', test).lower()\n",
    "        # print(name)  # camel_case_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "f = open(\"vtvhyundai.json\")\n",
    "vtvhyundai = json.load(f)\n",
    "f.close()\n",
    "f = open(\"vtvcabcrm.json\")\n",
    "vtvcabcrm = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"vtvcabcrm.json\")\n",
    "vtvcabcrm = json.load(f)\n",
    "f.close()\n",
    "res = []\n",
    "df = pd.DataFrame(vtvcabcrm)\n",
    "print(set(df['Gender'].to_list()))\n",
    "for profile in vtvcabcrm:\n",
    "    profile['UserId'] = profile.pop('CrmId')\n",
    "    profile['HomePhone'] = profile.pop('Phone')\n",
    "    profile['WorkPhone'] = None\n",
    "    profile['AddrHouseNo'] = profile.pop('HouseNo') if 'HouseNo' in profile else ''\n",
    "    profile['AddrStreetName'] = profile.pop('StreetName')\n",
    "    profile['AddrWard'] = profile.pop('Ward')\n",
    "    profile['AddrDistrict'] = profile.pop('District')\n",
    "    profile['AddrProvince'] = profile.pop('Province')\n",
    "    profile['AddrCountry'] = profile.pop('Country') if 'Country' in profile else 'Việt Nam'\n",
    "    profile['Firstname'] = profile.pop('Firstname')\n",
    "    profile['Lastname'] = profile.pop('Lastname') \n",
    "    profile['RegisterDate'] = '2022-01-01T00:00:00.00Z'\n",
    "    # profile['Gender'] = profile.pop('Gender')\n",
    "    if 'Gender' in profile:\n",
    "        if profile['Gender'] == 'Nam':\n",
    "            profile['Gender'] = 'Male'\n",
    "        if profile['Gender'] == 'Nữ':\n",
    "            profile['Gender'] = 'Female'\n",
    "    else:\n",
    "        profile['Gender'] = 'Not Define'\n",
    "    profile['DataSource'] = profile.pop('DataSource')\n",
    "    profile['MaritalStatus'] = 'Single'\n",
    "    del profile['Source']\n",
    "    del profile['EventId']\n",
    "    res.append(profile)\n",
    "print(res)\n",
    "\n",
    "with open('vtvcabcrm_formated.json', 'w') as f:\n",
    "    json.dump(res, f, indent=4, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = any(item in df_vtvhyundai['HomePhone'].to_list() for item in df_vtvcabcrm['HomePhone'].to_list())\n",
    "print(check)\n",
    "frames = [df_vtvcabcrm, df_vtvhyundai]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "data_provider = {\n",
    "    'vtvcabcrm':1,\n",
    "    'vtvhyundai':2\n",
    "}\n",
    "\n",
    "with open(\"vtv-hyundai_ingest-profile_20230307.json\", \"r\") as f:\n",
    "    source = f.read()\n",
    "with open(\"vtv-cab-crm_ingest-profile_20230307.json\", \"r\") as f:\n",
    "    new_source = f.read()\n",
    "\n",
    "source = [json.loads(row) for row in source.split('\\n') if row]\n",
    "new_source = [json.loads(row) for row in new_source.split('\\n') if row]\n",
    "# df_vtvhyundai = pd.DataFrame(vtvhyundai)\n",
    "# df_vtvcabcrm = pd.DataFrame(vtvcabcrm)\n",
    "source_homephone = {item['HomePhone']: item for item in source}\n",
    "print(source_homephone)\n",
    "# for item in new_source:\n",
    "#     if item['HomePhone'] not in source_homephone:\n",
    "#         source_homephone[item['HomePhone']] = item\n",
    "#     else:\n",
    "#         if data_provider[(item['DataSource'].lower()).replace(' ',\"\")] < data_provider[(source_homephone[item['HomePhone']]['DataSource'].lower()).replace(' ',\"\")]:\n",
    "#             for key, value in item.items():\n",
    "#                 if not value:\n",
    "#                     del item[key]\n",
    "#             source_homephone[item['HomePhone']].update(item)\n",
    "#         else:\n",
    "#             for key, value in item.items():\n",
    "#                 if key not in source_homephone[item['HomePhone']]:\n",
    "#                     source_homephone[item['HomePhone']][key] = value\n",
    "# result = list(source_homephone.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    'vtvcabcrm':1,\n",
    "    'vtvhyundai':2\n",
    "}\n",
    "b = {\n",
    "    'vtvcabcrm':1,\n",
    "    'vtvhyundai':2,\n",
    "    'vtvcabcrm123':1,\n",
    "    'vtvhyundai123':2\n",
    "}\n",
    "a.update(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tera_etl.utils.consolidate_utils import merge_user_profile_data\n",
    "with open(\"vtv-hyundai_ingest-profile_20230307.json\", \"r\") as f:\n",
    "    base_data = f.read()\n",
    "with open(\"vtv-cab-crm_ingest-profile_20230307.json\", \"r\") as f:\n",
    "    ingest_data = f.read()\n",
    "\n",
    "base_data = [json.loads(row) for row in base_data.split('\\n') if row]\n",
    "ingest_data = [json.loads(row) for row in ingest_data.split('\\n') if row]\n",
    "a = merge_user_profile_data(ingest_data, base_data)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PROVIDER_PRIORITIES = {\n",
    "    'vtvcabcrm': 1,\n",
    "    'vtvhyundai': 2,\n",
    "}\n",
    "\n",
    "CONSOLIDATE_KEY_PRIORITIES = {\n",
    "    'HomePhone': 1,\n",
    "    'UserId': 2,\n",
    "}\n",
    "import copy\n",
    "def get_consolidated_key(pre_data, item):\n",
    "    consolidated_key = ''\n",
    "    for k in CONSOLIDATE_KEY_PRIORITIES.keys():\n",
    "        if item[k] in pre_data[k]:\n",
    "            consolidated_key = k\n",
    "            break\n",
    "    return consolidated_key\n",
    "\n",
    "\n",
    "def get_index_item_by_type(pre_data, item, consolidated_key):\n",
    "    index_item_by_consolidated_key = {\n",
    "        'HomePhone': pre_data[consolidated_key][item[consolidated_key]],\n",
    "        'UserId': pre_data[consolidated_key][item[consolidated_key]],\n",
    "    }\n",
    "    return index_item_by_consolidated_key[consolidated_key]\n",
    "\n",
    "\n",
    "def merge_user_profile_data(ingest_data, base_data):\n",
    "    result = copy.deepcopy(base_data)\n",
    "    pre_data = {}\n",
    "    for k in CONSOLIDATE_KEY_PRIORITIES.keys():\n",
    "        pre_data[k]= {}\n",
    "    for i, item in enumerate(result):\n",
    "        for k in CONSOLIDATE_KEY_PRIORITIES.keys():\n",
    "            pre_data[k][item[k]] = i\n",
    "    for item in ingest_data:\n",
    "        consolidated_key = get_consolidated_key(pre_data, item)\n",
    "        if not consolidated_key:\n",
    "            result.append(item)\n",
    "            for k in CONSOLIDATE_KEY_PRIORITIES.keys():\n",
    "                pre_data[k][item[k]] = len(result)\n",
    "        else:\n",
    "            index_item = get_index_item_by_type(pre_data, item, consolidated_key)\n",
    "            if DATA_PROVIDER_PRIORITIES[(item['DataSource'].lower()).replace(' ', \"\")] <= DATA_PROVIDER_PRIORITIES[(result[index_item]['DataSource'].lower()).replace(' ', \"\")]:\n",
    "                for key, value in item.items():\n",
    "                    if value:\n",
    "                        result[index_item][key] = value\n",
    "            else:\n",
    "                for key, value in item.items():\n",
    "                    if key not in result[index_item] and value:\n",
    "                        result[index_item][key] = value\n",
    "    return result\n",
    "with open(\"vtv-hyundai_ingest-profile_20230307.json\", \"r\") as f:\n",
    "    base_data = f.read()\n",
    "with open(\"vtv-cab-crm_ingest-profile_20230307.json\", \"r\") as f:\n",
    "    ingest_data = f.read()\n",
    "\n",
    "base_data = [json.loads(row) for row in base_data.split('\\n') if row]\n",
    "ingest_data = [json.loads(row) for row in ingest_data.split('\\n') if row]\n",
    "result = merge_user_profile_data(ingest_data, base_data)\n",
    "print(result)\n",
    "# test = {}\n",
    "# for i, item in enumerate(result):\n",
    "#     test[item['HomePhone']] = i\n",
    "# print(len(test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"profile_ontv.json\")\n",
    "vtvcabcrm = json.load(f)\n",
    "f.close()\n",
    "res = []\n",
    "df = pd.DataFrame(vtvcabcrm)\n",
    "print(set(df['Gender'].to_list()))\n",
    "for profile in vtvcabcrm:\n",
    "    profile['UserId'] = profile.pop('CrmId')\n",
    "    profile['HomePhone'] = profile.pop('Phone')\n",
    "    profile['WorkPhone'] = None\n",
    "    profile['AddrHouseNo'] = profile.pop('HouseNo') if 'HouseNo' in profile else ''\n",
    "    profile['AddrStreetName'] = profile.pop('Street')\n",
    "    profile['AddrWard'] = profile.pop('Area')\n",
    "    profile['AddrDistrict'] = profile.pop('District') if 'District' in profile else ''\n",
    "    profile['AddrProvince'] = profile.pop('City')\n",
    "    profile['AddrCountry'] = profile.pop('Country') if 'Country' in profile else 'Việt Nam'\n",
    "    profile['Firstname'] = profile.pop('Firstname')\n",
    "    profile['Lastname'] = profile.pop('Lastname') \n",
    "    profile['RegisterDate'] = '2022-01-01T00:00:00.00Z'\n",
    "    if 'Gender' in profile:\n",
    "        if profile['Gender'] == 'Nam':\n",
    "            profile['Gender'] = 'Male'\n",
    "        if profile['Gender'] == 'Nữ':\n",
    "            profile['Gender'] = 'Female'\n",
    "    else:\n",
    "        profile['Gender'] = 'Not Define'\n",
    "    profile['DataSource'] = profile.pop('DataSource') if 'DataSource' in profile else ''\n",
    "    profile['MaritalStatus'] = 'Single'\n",
    "    del profile['PhoneId']\n",
    "    del profile['FullAddress']\n",
    "    if 'DOB' in profile: del profile['DOB']\n",
    "    if 'MOB' in profile: del profile['DOB']\n",
    "    if 'YOB' in profile: del profile['DOB']\n",
    "    if 'Birthday' in profile: del profile['DOB']\n",
    "    res.append(profile)\n",
    "\n",
    "\n",
    "with open('ontv_formated.json', 'w') as f:\n",
    "    json.dump(res, f, indent=4, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "VODNameWatched = ['one', 'two', 'three', 'four', 'five', None]\n",
    "Tag = ['one', 'two', 'three', 'four', 'five', None]\n",
    "Category = ['one', 'two', 'three', 'four', 'five', None]\n",
    "import uuid\n",
    "import json\n",
    "from random import randint\n",
    "import random\n",
    "for i in range(10000):\n",
    "    data = {\n",
    "        \"VODId\": str(uuid.uuid4()),\n",
    "        \"DeviceId\": str(uuid.uuid4()),\n",
    "        \"DurationWatched\": randint(100,100000),\n",
    "        \"VODNameWatched\": random.choice(VODNameWatched),\n",
    "        \"Tag\": random.choice(Tag),\n",
    "        \"Category\": random.choice(Category),\n",
    "    }\n",
    "    res.append(data)\n",
    "with open('/Users/mac/Desktop/user_watch_video.json', 'w') as f:\n",
    "    json.dump(res, f, indent=4, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# with open(\"/Users/admin/Documents/tera/data_test/data/vtvhyunhdai_profile.json\") as f:\n",
    "#     json_data = json.load(f)\n",
    "#     # Define the output CSV file name\n",
    "#     csv_file = \"output.csv\"\n",
    "\n",
    "#     # Define the CSV header row\n",
    "#     header = [key for key in vtvhyundai[0]]\n",
    "\n",
    "#     # Open the CSV file for writing\n",
    "#     with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames=header)\n",
    "#         # Write the header row\n",
    "#         writer.writeheader()\n",
    "#         # Loop through the JSON data and write each row to the CSV file\n",
    "#         for data in json_data:\n",
    "#             writer.writerow(data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open('/Users/admin/Documents/tera/data_test/data/data_formated_500000.json', encoding='utf-8') as inputfile:\n",
    "    df = pd.read_json(inputfile)\n",
    "\n",
    "# df['UserId'] = df['UserId'].astype(str)\n",
    "print(df.info())\n",
    "df.to_csv('/Users/admin/Documents/tera/data_test/data/ingestprofile_500000.csv', encoding='utf-8', index=False)\n",
    "# df_new = pd.read_csv('vtvhyundai_ingestprofile_20230328.csv')\n",
    "# print(df_new['UserId'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from memory_profiler import profile\n",
    "from smart_open import open\n",
    "bucket_name = 'sdlc-teracdp-ingest'\n",
    "object_key = 'batch-ingest/src=ontv/evt=ingestprofile/dt=20230329/ontv_ingestprofile_20230329.csv'\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"ap-southeast-1\",\n",
    ")\n",
    "# url = f's3://{os.path.join(bucket_name, object_key)}'\n",
    "@profile\n",
    "def test(s3_client, bucket_name, object_key):\n",
    "    url = f's3://{os.path.join(bucket_name, object_key)}'\n",
    "    with open(url, 'r', transport_params={'client': s3_client}) as out:\n",
    "        # chunk = pd.read_csv(out, dtype='str', chunksize=1000)\n",
    "        chunk = pd.read_csv(out, dtype='str', skiprows=range(5, 10), nrows=1000)\n",
    "        for row in chunk:\n",
    "            row = row.replace(np.nan, '').to_dict('records')\n",
    "            print(row)\n",
    "\n",
    "a = test(s3_client, bucket_name, object_key)\n",
    "# print(len(a))\n",
    "# print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_to_s3_object(s3_client, bucket_name, object_key, upload_id, new_data, part_number):\n",
    "    # Upload the new data as a part\n",
    "    response = s3_client.upload_part(\n",
    "        Bucket=bucket_name,\n",
    "        Key=object_key,\n",
    "        UploadId=upload_id,\n",
    "        PartNumber=part_number,\n",
    "        Body=new_data\n",
    "    )\n",
    "    return {\n",
    "        'ETag': f'{response[\"ETag\"]}',\n",
    "        'PartNumber': part_number,\n",
    "    }\n",
    "\n",
    "def get_multipart_upload_id(s3_client, bucket_name, object_key):\n",
    "    response = s3_client.create_multipart_upload(\n",
    "        Bucket=bucket_name,\n",
    "        Key=object_key\n",
    "    )\n",
    "    return response['UploadId']\n",
    "\n",
    "def complete_multipart_upload(s3_client, bucket_name, object_key, upload_id, parts):\n",
    "    # Complete the multipart upload\n",
    "    response = s3_client.complete_multipart_upload(\n",
    "        Bucket=bucket_name,\n",
    "        Key=object_key,\n",
    "        UploadId=upload_id,\n",
    "        MultipartUpload={\n",
    "            'Parts': parts\n",
    "        }\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tera_etl.quality_control as qc\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from memory_profiler import profile\n",
    "from smart_open import open\n",
    "bucket_name = 'sdlc-teracdp-ingest'\n",
    "data_source = 'ontv'\n",
    "event_name = 'ingestprofile'\n",
    "dt = '20230329'\n",
    "object_key = f'batch-ingest/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.csv'\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"ap-southeast-1\",\n",
    ")\n",
    "row_count = 0\n",
    "accept_count = 0\n",
    "reject_count = 0\n",
    "bucket_name_qc='sdlc-teracdp-data-quality'\n",
    "object_key_accept=f'accept_test/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.json'\n",
    "object_key_reject=f'reject_test/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.json'\n",
    "upload_id_accepted_data = get_multipart_upload_id(bucket_name_qc,object_key_accept)\n",
    "# upload_id_rejected_data = get_multipart_upload_id(bucket_name_qc,object_key_reject)\n",
    "parts_accept = []\n",
    "parts_reject = []\n",
    "part_number_accept = 0\n",
    "part_number_reject = 0\n",
    "offset = 0\n",
    "nrows=100000\n",
    "url = f's3://{os.path.join(bucket_name, object_key)}'\n",
    "while True:\n",
    "    with open(url, 'r', transport_params={'client': s3_client}) as out:\n",
    "        skiprows = 0 if offset == 0 else range(1, (offset*nrows + 1))\n",
    "        chunk = pd.read_csv(out, dtype='str', skiprows=skiprows, nrows=nrows)\n",
    "        rows = chunk.replace(np.nan, '').to_dict('records')\n",
    "        if len(rows) == 0:\n",
    "            break\n",
    "        \n",
    "        accepted_arr = []\n",
    "        rejected_arr = []\n",
    "        for row in rows:\n",
    "            row_count += 1\n",
    "            qc_result = qc.classify_data(data_chunk=row, schema_name='UserProfileSchema')\n",
    "            if qc_result['qc_type'] == qc.QualityControlResult.ACCEPTED:\n",
    "                accept_count +=1\n",
    "                accepted_arr.append(row)\n",
    "            elif qc_result['qc_type'] == qc.QualityControlResult.REJECTED:\n",
    "                reject_count +=1\n",
    "                rejected_arr.append(\n",
    "                    {\n",
    "                        'item': row,\n",
    "                        'errors': qc_result['errors']\n",
    "                    }\n",
    "                )\n",
    "                print(qc_result['errors'])\n",
    "            else:\n",
    "                raise Exception('Unimplemented QualityControl Type.')\n",
    "        offset += 1\n",
    "        # if accepted_arr:\n",
    "        #     part_number_accept+=1\n",
    "        #     res = append_data_to_s3_object(\n",
    "        #         s3_client=s3_client,\n",
    "        #         bucket_name=bucket_name_qc,\n",
    "        #         object_key=object_key_accept,\n",
    "        #         upload_id=upload_id_accepted_data,\n",
    "        #         part_number=part_number_accept,\n",
    "        #         new_data='\\n'.join([json.dumps(item, default=str, ensure_ascii=False) for item in accepted_arr]),\n",
    "        #     )\n",
    "        #     parts_accept.append(res)\n",
    "        # if rejected_arr:\n",
    "        #     res = append_data_to_s3_object(\n",
    "        #         s3_client=s3_client,\n",
    "        #         bucket_name=bucket_name_qc,\n",
    "        #         object_key=object_key_reject,\n",
    "        #         upload_id=upload_id_rejected_data,\n",
    "        #         part_number=offset,\n",
    "        #         new_data='\\n'.join([json.dumps(item, default=str, ensure_ascii=False) for item in rejected_arr]),\n",
    "        #     )\n",
    "        #     parts_reject.append(res)\n",
    "        print(f'NUMBER ROW: {row_count}')\n",
    "print(f'NUMBER: {accept_count}/{reject_count}')\n",
    "if accept_count > 0:\n",
    "    complete_multipart_upload(\n",
    "        s3_client=s3_client,\n",
    "        bucket_name=bucket_name_qc,\n",
    "        object_key=object_key_accept,\n",
    "        upload_id=upload_id_accepted_data,\n",
    "        parts=parts_accept,\n",
    "    )\n",
    "\n",
    "# complete_multipart_upload(\n",
    "#     s3_client=s3_client,\n",
    "#     bucket_name=bucket_name_qc,\n",
    "#     object_key=object_key_reject,\n",
    "#     upload_id=upload_id_rejected_data,\n",
    "#     parts=parts_reject,\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parts_accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "8000000\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "f = open(\"/Users/admin/Documents/tera/data_test/data/data_formated_500000.json\")\n",
    "vtvcabcrm = json.load(f)\n",
    "f.close()\n",
    "res =  copy.deepcopy(vtvcabcrm)\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    res += vtvcabcrm\n",
    "print(len(res))\n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv(f'/Users/admin/Documents/tera/data_test/data/ingestprofile_{len(res)}.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# with open('/Users/admin/Documents/tera/data_test/data/data_formated_5000000.json', 'w') as f:\n",
    "#     json.dump(res, f, indent=4, default=str, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from smart_open import open\n",
    "import json\n",
    "import tera_etl.normalise_data as nd\n",
    "bucket_name = 'sdlc-teracdp-data-normalised'\n",
    "data_source = 'ontv'\n",
    "event_name = 'ingestprofile'\n",
    "dt = '20230330'\n",
    "object_key = f'src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.json'\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"ap-southeast-1\",\n",
    ")\n",
    "vo = 0\n",
    "row_count = 0\n",
    "accepted_arr = []\n",
    "parts = []\n",
    "bucket_name_qc='sdlc-teracdp-data-quality'\n",
    "object_key_accept=f'accept/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.json'\n",
    "number_accepted_items = 499922\n",
    "upload_id = get_multipart_upload_id(\n",
    "    s3_client=s3_client,\n",
    "    bucket_name=bucket_name,\n",
    "    object_key=object_key,\n",
    ")\n",
    "for line in open(f's3://{os.path.join(bucket_name_qc, object_key_accept)}', 'r', transport_params={'client': s3_client}):\n",
    "    row_count +=1\n",
    "    accepted_arr.append(json.loads(line))\n",
    "    if (\n",
    "        row_count % nrows == 0\n",
    "        or row_count == number_accepted_items\n",
    "    ):\n",
    "        vo +=1\n",
    "        print(f'VOVOVO {vo}')\n",
    "        # normalised_arr = nd.normalise_data(accepted_arr, 'UserProfileSchema')\n",
    "        # accepted_arr = []\n",
    "        # res = append_data_to_s3_object(\n",
    "        #     s3_client=s3_client,\n",
    "        #     bucket_name=bucket_name,\n",
    "        #     object_key=object_key,\n",
    "        #     upload_id=upload_id,\n",
    "        #     part_number=(len(parts) + 1),\n",
    "        #     new_data= '\\n'.join([json.dumps(item, default=str, ensure_ascii=False) for item in normalised_arr])\n",
    "        # )\n",
    "        # parts.append(res)\n",
    "# complete_multipart_upload(\n",
    "#     s3_client=s3_client,\n",
    "#     bucket_name=bucket_name,\n",
    "#     object_key=object_key,\n",
    "#     upload_id=upload_id,\n",
    "#     parts=parts,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data_from_s3(s3_client, bucket_name, object_key):\n",
    "    try:\n",
    "        content_object = s3_client.get_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "        )\n",
    "        file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "        return file_content\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "rows_string = download_data_from_s3(\n",
    "    s3_client=s3_client,\n",
    "    bucket_name=bucket_name,\n",
    "    object_key=object_key,\n",
    ")\n",
    "print(rows_string)\n",
    "print(bucket_name)\n",
    "print(object_key)\n",
    "accepted_arr = [json.loads(row) for row in rows_string.split('\\n') if row]\n",
    "print(len(accepted_arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.dynamodb.conditions import And, Key\n",
    "from functools import reduce\n",
    "dynamodb_resource = boto3.resource(\n",
    "    'dynamodb',\n",
    "    aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"ap-southeast-1\",\n",
    ")\n",
    "def __scan_dynamo_for_data(dynamodb_resource, table_name, attribs: dict, expected_row_count):\n",
    "    table = dynamodb_resource.Table(table_name)    \n",
    "    response = table.scan(\n",
    "        FilterExpression=(\n",
    "            reduce(And, [Key(k).eq(v) for k, v in attribs.items()])\n",
    "        )\n",
    "    )\n",
    "    rows = response.get('Items')\n",
    "    assert len(rows) == expected_row_count, f'Query from {table_name} with {attribs} returns unexpected value: expect {expected_row_count}, returns {len(rows)}'\n",
    "    \n",
    "    return rows[:expected_row_count]\n",
    "data_schema = __scan_dynamo_for_data(\n",
    "        dynamodb_resource=dynamodb_resource,\n",
    "        table_name='SDLC-SchemaRegistry',\n",
    "        attribs={'SchemaName': 'UserProfileSchema', 'SchemaVersion': '1'},\n",
    "        expected_row_count=1\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported field type: datetime",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m         schema_dict[field_name] \u001b[39m=\u001b[39m pyorc\u001b[39m.\u001b[39mFloat()\n\u001b[1;32m     19\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported field type: \u001b[39m\u001b[39m{\u001b[39;00mfield_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m schema \u001b[39m=\u001b[39m pyorc\u001b[39m.\u001b[39mStruct(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mschema_dict)\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported field type: datetime"
     ]
    }
   ],
   "source": [
    "import pyorc\n",
    "import pandas as pd\n",
    "df_dict = {}\n",
    "schema_dict = {}\n",
    "for field in data_schema['Structures']['Fields']:\n",
    "    field_name = field[\"Name\"]\n",
    "    field_type = field[\"FieldType\"]\n",
    "    nullable = field[\"Nullable\"]\n",
    "    \n",
    "    if field_type.upper() == \"STRING\":\n",
    "        df_dict[field_name] = pd.StringDtype()\n",
    "        schema_dict[field_name] = pyorc.String()\n",
    "    elif field_type.upper() == \"INT\":\n",
    "        df_dict[field_name] = pd.Int32Dtype()\n",
    "        schema_dict[field_name] = pyorc.Int()\n",
    "    elif field_type.upper() == \"FLOAT\":\n",
    "        df_dict[field_name] = pd.Float32Dtype()\n",
    "        schema_dict[field_name] = pyorc.Float()\n",
    "    elif field_type.upper() == \"DATETIME\":\n",
    "        df_dict[field_name] = pd.DatetimeTZDtype()\n",
    "        schema_dict[field_name] = pyorc.Float()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported field type: {field_type}\")\n",
    "schema = pyorc.Struct(**schema_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tera_etl.quality_control as qc\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "from memory_profiler import profile\n",
    "from smart_open import open\n",
    "bucket_name = 'sdlc-teracdp-ingest'\n",
    "data_source = 'ontv'\n",
    "event_name = 'ingestprofile'\n",
    "dt = '20230329'\n",
    "object_key = f'batch-ingest/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.csv'\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"ap-southeast-1\",\n",
    ")\n",
    "row_count = 0\n",
    "accept_count = 0\n",
    "reject_count = 0\n",
    "bucket_name_qc='sdlc-teracdp-data-quality'\n",
    "object_key_accept=f'accept_test/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.json'\n",
    "object_key_reject=f'reject_test/src={data_source}/evt={event_name}/dt={dt}/{data_source}_{event_name}_{dt}.json'\n",
    "upload_id_accepted_data = get_multipart_upload_id(bucket_name_qc,object_key_accept)\n",
    "# upload_id_rejected_data = get_multipart_upload_id(bucket_name_qc,object_key_reject)\n",
    "parts_accept = []\n",
    "parts_reject = []\n",
    "part_number_accept = 0\n",
    "part_number_reject = 0\n",
    "offset = 0\n",
    "nrows=100000\n",
    "url = f's3://{os.path.join(bucket_name, object_key)}'\n",
    "while True:\n",
    "    with open(url, 'r', transport_params={'client': s3_client}) as out:\n",
    "        skiprows = 0 if offset == 0 else range(1, (offset*nrows + 1))\n",
    "        chunk = pd.read_csv(out, dtype='str', skiprows=skiprows, nrows=nrows)\n",
    "        rows = chunk.replace(np.nan, '').to_dict('records')\n",
    "        if len(rows) == 0:\n",
    "            break\n",
    "        \n",
    "        accepted_arr = []\n",
    "        rejected_arr = []\n",
    "        for row in rows:\n",
    "            row_count += 1\n",
    "            qc_result = qc.classify_data(data_chunk=row, schema_name='UserProfileSchema')\n",
    "            if qc_result['qc_type'] == qc.QualityControlResult.ACCEPTED:\n",
    "                accept_count +=1\n",
    "                accepted_arr.append(row)\n",
    "            elif qc_result['qc_type'] == qc.QualityControlResult.REJECTED:\n",
    "                reject_count +=1\n",
    "                rejected_arr.append(\n",
    "                    {\n",
    "                        'item': row,\n",
    "                        'errors': qc_result['errors']\n",
    "                    }\n",
    "                )\n",
    "                print(qc_result['errors'])\n",
    "            else:\n",
    "                raise Exception('Unimplemented QualityControl Type.')\n",
    "        offset += 1\n",
    "        \n",
    "        print(f'NUMBER ROW: {row_count}')\n",
    "print(f'NUMBER: {accept_count}/{reject_count}')\n",
    "if accept_count > 0:\n",
    "    complete_multipart_upload(\n",
    "        s3_client=s3_client,\n",
    "        bucket_name=bucket_name_qc,\n",
    "        object_key=object_key_accept,\n",
    "        upload_id=upload_id_accepted_data,\n",
    "        parts=parts_accept,\n",
    "    )\n",
    "\n",
    "# complete_multipart_upload(\n",
    "#     s3_client=s3_client,\n",
    "#     bucket_name=bucket_name_qc,\n",
    "#     object_key=object_key_reject,\n",
    "#     upload_id=upload_id_rejected_data,\n",
    "#     parts=parts_reject,\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"AKIAZQSLELDPHSPH6LFK\",\n",
    "    aws_secret_access_key=\"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\",\n",
    "    region_name=\"ap-southeast-1\",\n",
    ")\n",
    "response = s3.put_object(\n",
    "    Bucket='sdlc-teracdp-dags',\n",
    "    Key='test/test.hql',\n",
    "    Body='kokokokokokokokok',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22cf4460807183c541d2709e0ea5f9a221822f7b4c3fb4be66d8f6254d7f8908"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
