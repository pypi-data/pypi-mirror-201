# -*- coding: utf-8 -*-
BOT_NAME = 'Scrapy_Prj'

SPIDER_MODULES = ['Scrapy_Prj.spiders']
NEWSPIDER_MODULE = 'Scrapy_Prj.spiders'

SPIDER_LOADER_CLASS = 'scrapy.teamloader.SpiderLoader'

########################################################
# 以下参数选配在custom_settings中                        #
# 对于scrapy.settings.default_settings中默认参数不用配置 #
########################################################

# Override the default request headers:
# DEFAULT_REQUEST_HEADERS = {
#     'user-agent': "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0",
# }

# UABROWSER = 'chrome' #"opera"/"firefox"/"internetexplorer"/"safari"/"random"

# 配置item,下载字段不同域名对应的请求头
# ITEM_HEADER = {"4donline.ihs.com":{
#         'Host': '4donline.ihs.com',
#         'Referer': 'https://www.xxxxx.com/',
#         'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'
#     },
#               "datasheet.datasheetarchive.com":{
#         'Host': 'datasheet.datasheetarchive.com',
#         'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'
#     }


# FILE_PIPE_CONFIG = {
#     'OssPDFPipeline':
#         {'FILES_OSS_URLS_FIELD': 'raw_pdf_url',
#          'FILES_OSS_RESULT_FIELD': 'pdf_url',
#          'RESOURCE_CLASSNAME': 'FactoryMaterialItem',
#          'FILTER_PDF_PAGE': True,  # 是否过滤处理pdf
#          'DEL_PDF_CHECK_NUM': 3,  # 从后向前检查多少页pdf
#          'DEL_PDF_ACCORDING_TO': {'and': ['This datasheet', 'for electronic components.'],  # 逻辑关系
#                                   'not': ['BC03151 ', ''],  # 逻辑关系
#                                   }},
#     'OssFilesPipeline':
#         {'FILES_OSS_URLS_FIELD': 'raw_pdf_url',
#          'FILES_OSS_RESULT_FIELD': 'pdf_url',
#          'RESOURCE_CLASSNAME': 'FactoryMaterialItem',
#          'FILTER_PDF_PAGE': True,  # 是否过滤处理pdf
#          'DEL_PDF_CHECK_NUM': 3,  # 从后向前检查多少页pdf
#          'DEL_PDF_ACCORDING_TO': {'and': ['This datasheet', 'for electronic components.'],  # 逻辑关系
#                                   'not': ['BC03151 ', ''],  # 逻辑关系
#                                   }},
#     'OssFilesPipelineBak':  # 于此类似,还有 OssFilesPipelineBakBak 类做备用
#         {'FILES_OSS_URLS_FIELD': 'raw_pdf_url',
#          'FILES_OSS_RESULT_FIELD': 'pdf_url',
#          'RESOURCE_CLASSNAME': 'FactoryPlanItem',
#          'FILTER_PDF_PAGE': False,  # 是否过滤处理pdf
#          'DEL_PDF_CHECK_NUM': 3,
#          'DEL_PDF_ACCORDING_TO': {'and': ['关键词', '关键词'],  # 逻辑关系
#                                   'not': ['关键词', '关键词'],  # 逻辑关系
#                                   }},
#     'OssImagesPipeline':  # 于此类似,还有 OssImagesPipelineBakBak 类做备用
#         {'FILES_OSS_URLS_FIELD': 'raw_img_url',
#          'FILES_OSS_RESULT_FIELD': 'img_url',
#          'RESOURCE_CLASSNAME': 'FactoryMaterialItem'}
# }



# Configure maximum concurrent requests performed by Scrapy (default: 16)
# CONCURRENT_REQUESTS = 8
# CONCURRENT_REQUESTS_PER_DOMAIN = 2  # minicircuits
# CONCURRENT_REQUESTS_PER_IP = 1
# Configure a delay for requests for the same website (default: 0)
# DOWNLOAD_DELAY = 0.5

# Disable cookies (enabled by default)
# COOKIES_ENABLED = True

# 资讯增量爬虫寿命设置
# CLOSESPIDER_TIMEOUT = 300  # 根据运行时常主动停止 doc url:https://doc.scrapy.org/en/latest/topics/extensions.html?highlight=CLOSESPIDER_TIMEOUT
# CLOSESPIDER_ITEMCOUNT = 100 # 根据item数量...
# CLOSESPIDER_PAGECOUNT = 100 # 根据请求量...
# CLOSESPIDER_ERRORCOUNT = 10 # 根据错误量...
# CLOSESPIDER_ERROR_STATUS = (429,20) # (<status_code>,<number>)根据错误状态码量...

# 后进先出，深度优先
# DEPTH_PRIORITY = -1
# SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'
# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'

# 先进先出，广度优先
# DEPTH_PRIORITY = 1
# SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'
# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'

# log
# from datetime import datetime
# dd = datetime.today()
# LOG_ENABLED = True
# LOG_ENCODING = 'UTF-8'
# LOG_FILE = 'log/{}_{}_{}.log'.format(dd.year, dd.month, dd.day)
# LOG_LEVEL = 'ERROR'

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'